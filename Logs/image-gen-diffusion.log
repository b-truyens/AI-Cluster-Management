Open sidebar
Image Generation with Diffusion

Plugins
•
Enabled plugins:
ScholarAI logo
KeyMate.AI Search logo
Wolfram logo

User
from diffusers import StableDiffusionOnnxPipeline

pipe = StableDiffusionOnnxPipeline.from_pretrained("./stable_diffusion_onnx", provider="DmlExecutionProvider")

prompt = "A happy celebrating robot on a mountaintop, happy, landscape, dramatic lighting, art by artgerm greg rutkowski alphonse mucha, 4k uhd'"

image = pipe(prompt).images[0]

image.save("output.png")
(.pt2) boris@sensei:/data/models$ cat gen2.py
from diffusers import DiffusionPipeline
import torch

pipe = DiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-xl-base-1.0", torch_dtype=torch.float16, use_safetensors=True, variant="fp16")


# if using torch < 2.0
# pipe.enable_xformers_memory_efficient_attention()

prompt = "An astronaut riding a green horse"

images = pipe(prompt=prompt).images[0]


ChatGPT
It looks like you've shared two code snippets related to generating images using diffusion models. The first snippet is using an ONNX pipeline to generate an image based on a prompt, and the second snippet is using a PyTorch-based pipeline to generate an image with a different prompt.

Is there something specific you would like to know or ask about these code snippets? Whether it's an explanation, assistance with an error, or something else, please let me know how I can help!




User
(.pt2) boris@sensei:/data/models$ nano gen.py
(.pt2) boris@sensei:/data/models$ python gen.py
  File "/data/models/gen.py", line 3
    pipe = StableDiffusionOnnxPipeline.from_pretrained(“./stable_diffusion_onnx”, provider=”DmlExecutionProvider”)
                                                       ^
SyntaxError: invalid character '“' (U+201C)
(.pt2) boris@sensei:/data/models$ nano gen.py
(.pt2) boris@sensei:/data/models$ nano gen.py
(.pt2) boris@sensei:/data/models$ python gen.py
  File "/data/models/gen.py", line 9
    image.save(“output.png”)
               ^
SyntaxError: invalid character '“' (U+201C)
(.pt2) boris@sensei:/data/models$ nano gen.py

ChatGPT
It seems like you're encountering a syntax error in your Python code due to the use of smart quotes (“ and ”) instead of standard ASCII quotes ("). This is a common issue when copying and pasting code from certain text editors or websites that automatically convert standard quotes to smart quotes.

You can fix this error by replacing the smart quotes with standard quotes in your code. Here's the corrected line from the first error:

python
Copy code
pipe = StableDiffusionOnnxPipeline.from_pretrained("./stable_diffusion_onnx", provider="DmlExecutionProvider")
And here's the corrected line from the second error:

python
Copy code
image.save("output.png")
Make sure to replace all instances of smart quotes in your code with standard quotes, and then you should be able to run the script without any syntax errors. If you have any further questions or need additional assistance, please let me know!




User
Illegal instruction (core dumped)


ChatGPT
The "Illegal instruction (core dumped)" error typically indicates that the program is trying to execute an instruction that is not supported by the CPU. This can happen for various reasons, such as:

Compiled Code for a Different Architecture: If the code or libraries you are using were compiled for a different CPU architecture or with instructions that your CPU doesn't support, you may encounter this error.

Using Unsupported Features: If you are using specific features or instructions that are not supported by your CPU (e.g., certain SIMD instructions), this error may occur.

Corrupted Binary or Library: If the binary or a library it depends on has become corrupted, this error might be triggered.

Here are some general steps you can take to troubleshoot the issue:

Check Compatibility: Ensure that the libraries and models you are using are compatible with your system's architecture and CPU. If you are using precompiled binaries, make sure they are built for your specific CPU or a compatible one.

Reinstall Libraries: If you suspect that a library might be corrupted or improperly installed, try reinstalling it.

Update System and Libraries: Make sure your system and all the libraries you are using are up to date. Sometimes, updating to the latest version can resolve compatibility issues.

Inspect the Core Dump: If you are comfortable with debugging, you can analyze the core dump using a tool like gdb to get more information about where and why the illegal instruction is occurring.

Consult Documentation: Check the documentation of the libraries and models you are using to see if there are any specific requirements or known issues related to your CPU or system configuration.

Use a Different Build: If you are building the code yourself, try using different build options or compiler flags that are compatible with your CPU.

Contact Support: If you are using a commercial product or library, consider reaching out to their support with details about your system and the error.

Without more specific information about the code, libraries, and system you are using, it's challenging to provide a more targeted solution. If you can provide more details, I may be able to assist you further!




User
(.pt2) boris@sensei:/data/models$ python gen.py
Illegal instruction (core dumped)
(.pt2) boris@sensei:/data/models$ nano test.py
(.pt2) boris@sensei:/data/models$ python test.py
Traceback (most recent call last):
  File "/data/models/test.py", line 1, in <module>
    import tensorflow as tf
ModuleNotFoundError: No module named 'tensorflow'
(.pt2) boris@sensei:/data/models$ cat test.py 
import tensorflow as tf
tf.test.is_gpu_available()


ChatGPT
It looks like you're encountering two different issues:

Illegal instruction (core dumped): This error occurred when running gen.py, and as mentioned earlier, it could be related to an incompatibility with your CPU or a corrupted binary/library.

ModuleNotFoundError: No module named 'tensorflow': This error occurred when running test.py, and it indicates that TensorFlow is not installed in the environment where you're running the script.

Here's how you can address the second issue:

Installing TensorFlow
You can install TensorFlow by running the following command in your terminal:

bash
Copy code
pip install tensorflow
If you're using a virtual environment, make sure it's activated before running the command.

Checking GPU Availability
Once TensorFlow is installed, you can run the test.py script to check if a GPU is available. Note that the tf.test.is_gpu_available() method is deprecated in recent versions of TensorFlow. You can use the following code instead:

python
Copy code
import tensorflow as tf
print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))
This will print the number of available GPUs.

Addressing the Illegal Instruction Error
For the "Illegal instruction" error in gen.py, you may need to provide more details about the code, libraries, and system configuration for a more targeted solution. If you can share more information, I'll do my best to assist you further!




User
how do I setup a fresh virtual environment to try and run stable diffusion in?

ChatGPT
Sure! Setting up a fresh virtual environment is a great way to isolate dependencies and troubleshoot issues. Here's a step-by-step guide to creating a virtual environment and installing the necessary packages to run stable diffusion:

Install Virtualenv (if not already installed):

bash
Copy code
pip install virtualenv
Create a Virtual Environment:
Navigate to the directory where you want to create the virtual environment and run:

bash
Copy code
virtualenv stable_diffusion_env
Activate the Virtual Environment:

On Windows:
bash
Copy code
stable_diffusion_env\Scripts\activate
On macOS and Linux:
bash
Copy code
source stable_diffusion_env/bin/activate
Install Required Libraries:
Once the virtual environment is activated, you can install the required libraries. For stable diffusion, you might need TensorFlow, PyTorch, and other dependencies. You can install them using pip:

bash
Copy code
pip install tensorflow torch
If there are specific versions or other libraries required, make sure to install them as well.

Run Your Code:
With the virtual environment activated and the necessary libraries installed, you can run your code:

bash
Copy code
python gen.py
Deactivate the Virtual Environment:
When you're done, you can deactivate the virtual environment by running:

bash
Copy code
deactivate
By isolating the environment and installing only the required dependencies, you can minimize conflicts and potential issues. If you encounter any problems or need further assistance, please let me know!




User
I'm running ubuntu 22.04 desktop with a NViDIA GTX 770 and an AMD Vega 56 8gb. The CPU is Ryzen 5 3600. Bare metal is running debian proxmox with a PCIcpasstrough VM which has the GPUS routed to it.

ChatGPT
Thank you for providing the details about your system configuration. Based on the information, here's a more specific guide to setting up a virtual environment and running stable diffusion on Ubuntu 22.04 with your hardware:

Update System Packages:

bash
Copy code
sudo apt update
sudo apt upgrade
Install Python and Virtualenv (if not already installed):

bash
Copy code
sudo apt install python3 python3-venv python3-pip
Create a Virtual Environment:

bash
Copy code
python3 -m venv stable_diffusion_env
Activate the Virtual Environment:

bash
Copy code
source stable_diffusion_env/bin/activate
Install TensorFlow with GPU Support:
Since you have NVIDIA GPUs, you'll want to install TensorFlow with GPU support:

bash
Copy code
pip install tensorflow-gpu
Install PyTorch with CUDA Support:
You can install PyTorch with CUDA support using the appropriate command from the PyTorch website.

Install Other Required Libraries:
Install any other required libraries for stable diffusion.

Run Your Code:

bash
Copy code
python gen.py
Deactivate the Virtual Environment:

bash
Copy code
deactivate
Additional Considerations:
NVIDIA Drivers: Make sure you have the appropriate NVIDIA drivers installed for your GTX 770. You can use the nvidia-smi command to check the driver status.

AMD GPU Support: If you plan to use the AMD Vega 56 GPU with TensorFlow, you may need to look into ROCm, AMD's platform for deep learning. Note that TensorFlow's support for AMD GPUs may be more limited.

Virtual Machine Configuration: Since you mentioned using PCI passthrough in a VM, ensure that the GPUs are correctly passed through and accessible to the guest OS.

By following these steps, you should be able to set up a virtual environment and run stable diffusion on your system. If you encounter any specific errors or need further assistance, please provide more details, and I'll do my best to help!




User
(stable_diffusion_env) boris@sensei:~$ lspci | grep AMD
00:1b.0 VGA compatible controller: Advanced Micro Devices, Inc. [AMD/ATI] Vega 10 XL/XT [Radeon RX Vega 56/64] (rev c3)
00:1b.1 Audio device: Advanced Micro Devices, Inc. [AMD/ATI] Vega 10 HDMI Audio [Radeon Vega 56/64]
(stable_diffusion_env) boris@sensei:~$ lspci | grep 770
00:11.0 VGA compatible controller: NVIDIA Corporation GK104 [GeForce GTX 770] (rev a1)
(stable_diffusion_env) boris@sensei:~$ 



ChatGPT
Great! It looks like both the AMD Vega 56 and NVIDIA GTX 770 GPUs are recognized by your system.

Here's what you can do next to ensure that they are set up properly for deep learning tasks:

For NVIDIA GTX 770:
Install NVIDIA CUDA Toolkit: If you haven't already, you'll need to install the CUDA toolkit to leverage the NVIDIA GPU. You can follow the instructions on the official NVIDIA website for Ubuntu.

Install cuDNN: The CUDA Deep Neural Network library (cuDNN) is often required for deep learning frameworks. You can download it from the NVIDIA cuDNN page.

Verify NVIDIA GPU with TensorFlow: Once everything is installed, you can verify that TensorFlow recognizes the NVIDIA GPU by running the following Python code:

python
Copy code
import tensorflow as tf
print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))
For AMD Vega 56:
Install ROCm: ROCm is AMD's platform for deep learning on Radeon GPUs. You can follow the official ROCm installation guide for Ubuntu.

Verify AMD GPU with TensorFlow: AMD's support for TensorFlow is provided through a ROCm-optimized version of TensorFlow. You can install it and verify the GPU as described in the ROCm TensorFlow documentation.

General Considerations:
Virtual Environment: Make sure to activate your virtual environment (source stable_diffusion_env/bin/activate) before installing any Python packages.

Compatibility: Check the compatibility of the CUDA, cuDNN, and ROCm versions with the version of TensorFlow or other deep learning frameworks you are using.

Multiple GPUs: If you plan to use both GPUs simultaneously, you may need to configure your deep learning code to handle multi-GPU training. This can vary depending on the framework and model you are using.

By following these steps, you should be able to set up both GPUs for deep learning tasks. If you encounter any issues or have specific questions, please let me know!




User
(stable_diffusion_env) boris@sensei:~$ pip install tensorflow-gpu
Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple
Collecting tensorflow-gpu
  Downloading tensorflow-gpu-2.12.0.tar.gz (2.6 kB)
  Preparing metadata (setup.py) ... done
Collecting python_version>"3.7"
  Downloading python_version-0.0.2-py2.py3-none-any.whl (3.4 kB)
Using legacy 'setup.py install' for tensorflow-gpu, since package 'wheel' is not installed.
Installing collected packages: python_version, tensorflow-gpu
  Running setup.py install for tensorflow-gpu ... error
  error: subprocess-exited-with-error
  
  × Running setup.py install for tensorflow-gpu did not run successfully.
  │ exit code: 1
  ╰─> [18 lines of output]
      Traceback (most recent call last):
        File "<string>", line 2, in <module>
        File "<pip-setuptools-caller>", line 34, in <module>
        File "/tmp/pip-install-lzzjobxd/tensorflow-gpu_9c07cee21077436c8c7ed71c389ffb27/setup.py", line 37, in <module>
          raise Exception(TF_REMOVAL_WARNING)
      Exception:
      
      =========================================================
      The "tensorflow-gpu" package has been removed!
      
      Please install "tensorflow" instead.
      
      Other than the name, the two packages have been identical
      since TensorFlow 2.1, or roughly since Sep 2019. For more
      information, see: pypi.org/project/tensorflow-gpu
      =========================================================
      
      
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
error: legacy-install-failure

× Encountered error while trying to install package.
╰─> tensorflow-gpu

note: This is an issue with the package mentioned above, not pip.
hint: See above for output from the failure.
(stable_diffusion_env) boris@sensei:~$ 


ChatGPT
It looks like you encountered an error because the tensorflow-gpu package has been removed, and you should install the tensorflow package instead. Starting from TensorFlow 2.1, the GPU support is included in the main tensorflow package, and there's no need to install a separate tensorflow-gpu package.

You can install TensorFlow with the following command:

bash
Copy code
pip install tensorflow
Once installed, TensorFlow will automatically use the GPU if it's available and compatible. You can verify that TensorFlow recognizes your GPU by running the following Python code:

python
Copy code
import tensorflow as tf
print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))
This should print the number of available GPUs. If everything is set up correctly, you should be able to proceed with running your stable diffusion code.

If you encounter any further issues or have additional questions, please let me know!




User
(stable_diffusion_env) boris@sensei:/data/models$ 

ChatGPT
It looks like your message was cut off. Are you facing any issues or do you have any questions about the process so far? If everything is working as expected, that's great! If not, please provide more details or ask any questions you may have, and I'll be happy to assist you further.




User
I switched directory to a mounted /data /models folder

ChatGPT
Great! It seems like you've switched to the directory where your models and data are located. You can now proceed with running your code or any other tasks you need to perform in that directory.

If you have any specific questions or need further assistance with anything related to your project, please don't hesitate to ask. I'm here to help!




User
Step 2: Download and install DirectML Onnx runtime
Next, download a build of Microsoft’s DirectML Onnx runtime. You can get it from here. Download the package that corresponds to your installed Python version. Once downloaded, use pip to install the package. Make sure you use the –force-reinstall flag to override some previously installed dependencies.

ChatGPT
It looks like you're planning to use Microsoft's DirectML ONNX runtime, which is a great choice for running ONNX models on Windows with DirectX 12.

Since you mentioned that you're using Ubuntu, please note that DirectML is primarily designed for Windows. However, you can still run ONNX models on Ubuntu using the standard ONNX Runtime.

Here's how you can install the ONNX Runtime on Ubuntu:

Activate Your Virtual Environment (if not already activated):

bash
Copy code
source stable_diffusion_env/bin/activate
Install ONNX Runtime:

bash
Copy code
pip install onnxruntime
Optional: Install ONNX Runtime with GPU Support (for NVIDIA GPUs):

bash
Copy code
pip install onnxruntime-gpu
If you specifically need DirectML for some reason and must run it on Ubuntu, you might need to consider using a Windows virtual machine or container with the necessary DirectX 12 support.

If you have any specific requirements or questions about this process, please let me know, and I'll do my best to assist you!




User
Requirement already satisfied: packaging in /home/boris/stable_diffusion_env/lib/python3.10/site-packages (from onnxruntime) (23.1)
Requirement already satisfied: protobuf in /home/boris/stable_diffusion_env/lib/python3.10/site-packages (from onnxruntime) (4.24.0)
Collecting humanfriendly>=9.1
  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.8/86.8 KB 1.2 MB/s eta 0:00:00
Requirement already satisfied: mpmath>=0.19 in /home/boris/stable_diffusion_env/lib/python3.10/site-packages (from sympy->onnxruntime) (1.3.0)
Installing collected packages: humanfriendly, coloredlogs, onnxruntime
Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.15.1
(stable_diffusion_env) boris@sensei:/data/models$ nano hf.py
(stable_diffusion_env) boris@sensei:/data/models$ python hf.py
Traceback (most recent call last):
  File "/data/models/hf.py", line 20, in <module>
    import onnx
ModuleNotFoundError: No module named 'onnx'
(stable_diffusion_env) boris@sensei:/data/models$ cat hf.py 
# Copyright 2023 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import argparse
import os
import shutil
from pathlib import Path

import onnx
import torch
from packaging import version
from torch.onnx import export

from diffusers import OnnxRuntimeModel, OnnxStableDiffusionPipeline, StableDiffusionPipeline


is_torch_less_than_1_11 = version.parse(version.parse(torch.__version__).base_version) < version.parse("1.11")


def onnx_export(
    model,
    model_args: tuple,
    output_path: Path,
    ordered_input_names,
    output_names,
    dynamic_axes,
    opset,
    use_external_data_format=False,
):
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # PyTorch deprecated the `enable_onnx_checker` and `use_external_data_format` arguments in v1.11,
    # so we check the torch version for backwards compatibility
    if is_torch_less_than_1_11:
        export(
            model,
            model_args,
            f=output_path.as_posix(),
            input_names=ordered_input_names,
            output_names=output_names,
            dynamic_axes=dynamic_axes,
            do_constant_folding=True,
            use_external_data_format=use_external_data_format,
            enable_onnx_checker=True,
            opset_version=opset,
        )
    else:
        export(
            model,
            model_args,
            f=output_path.as_posix(),
            input_names=ordered_input_names,
            output_names=output_names,
            dynamic_axes=dynamic_axes,
            do_constant_folding=True,
            opset_version=opset,
        )


@torch.no_grad()
def convert_models(model_path: str, output_path: str, opset: int, fp16: bool = False):
    dtype = torch.float16 if fp16 else torch.float32
    if fp16 and torch.cuda.is_available():
        device = "cuda"
    elif fp16 and not torch.cuda.is_available():
        raise ValueError("`float16` model export is only supported on GPUs with CUDA")
    else:
        device = "cpu"
    pipeline = StableDiffusionPipeline.from_pretrained(model_path, torch_dtype=dtype).to(device)
    output_path = Path(output_path)

    # TEXT ENCODER
    num_tokens = pipeline.text_encoder.config.max_position_embeddings
    text_hidden_size = pipeline.text_encoder.config.hidden_size
    text_input = pipeline.tokenizer(
        "A sample prompt",
        padding="max_length",
        max_length=pipeline.tokenizer.model_max_length,
        truncation=True,
        return_tensors="pt",
    )
    onnx_export(
        pipeline.text_encoder,
        # casting to torch.int32 until the CLIP fix is released: https://github.com/huggingface/transformers/pull/18515/files
        model_args=(text_input.input_ids.to(device=device, dtype=torch.int32)),
        output_path=output_path / "text_encoder" / "model.onnx",
        ordered_input_names=["input_ids"],
        output_names=["last_hidden_state", "pooler_output"],
        dynamic_axes={
            "input_ids": {0: "batch", 1: "sequence"},
        },
        opset=opset,
    )
    del pipeline.text_encoder

    # UNET
    unet_in_channels = pipeline.unet.config.in_channels
    unet_sample_size = pipeline.unet.config.sample_size
    unet_path = output_path / "unet" / "model.onnx"
    onnx_export(
        pipeline.unet,
        model_args=(
            torch.randn(2, unet_in_channels, unet_sample_size, unet_sample_size).to(device=device, dtype=dtype),
            torch.randn(2).to(device=device, dtype=dtype),
            torch.randn(2, num_tokens, text_hidden_size).to(device=device, dtype=dtype),
            False,
        ),
        output_path=unet_path,
        ordered_input_names=["sample", "timestep", "encoder_hidden_states", "return_dict"],
        output_names=["out_sample"],  # has to be different from "sample" for correct tracing
        dynamic_axes={
            "sample": {0: "batch", 1: "channels", 2: "height", 3: "width"},
            "timestep": {0: "batch"},
            "encoder_hidden_states": {0: "batch", 1: "sequence"},
        },
        opset=opset,
        use_external_data_format=True,  # UNet is > 2GB, so the weights need to be split
    )
    unet_model_path = str(unet_path.absolute().as_posix())
    unet_dir = os.path.dirname(unet_model_path)
    unet = onnx.load(unet_model_path)
    # clean up existing tensor files
    shutil.rmtree(unet_dir)
    os.mkdir(unet_dir)
    # collate external tensor files into one
    onnx.save_model(
        unet,
        unet_model_path,
        save_as_external_data=True,
        all_tensors_to_one_file=True,
        location="weights.pb",
        convert_attribute=False,
    )
    del pipeline.unet

    # VAE ENCODER
    vae_encoder = pipeline.vae
    vae_in_channels = vae_encoder.config.in_channels
    vae_sample_size = vae_encoder.config.sample_size
    # need to get the raw tensor output (sample) from the encoder
    vae_encoder.forward = lambda sample, return_dict: vae_encoder.encode(sample, return_dict)[0].sample()
    onnx_export(
        vae_encoder,
        model_args=(
            torch.randn(1, vae_in_channels, vae_sample_size, vae_sample_size).to(device=device, dtype=dtype),
            False,
        ),
        output_path=output_path / "vae_encoder" / "model.onnx",
        ordered_input_names=["sample", "return_dict"],
        output_names=["latent_sample"],
        dynamic_axes={
            "sample": {0: "batch", 1: "channels", 2: "height", 3: "width"},
        },
        opset=opset,
    )

    # VAE DECODER
    vae_decoder = pipeline.vae
    vae_latent_channels = vae_decoder.config.latent_channels
    vae_out_channels = vae_decoder.config.out_channels
    # forward only through the decoder part
    vae_decoder.forward = vae_encoder.decode
    onnx_export(
        vae_decoder,
        model_args=(
            torch.randn(1, vae_latent_channels, unet_sample_size, unet_sample_size).to(device=device, dtype=dtype),
            False,
        ),
        output_path=output_path / "vae_decoder" / "model.onnx",
        ordered_input_names=["latent_sample", "return_dict"],
        output_names=["sample"],
        dynamic_axes={
            "latent_sample": {0: "batch", 1: "channels", 2: "height", 3: "width"},
        },
        opset=opset,
    )
    del pipeline.vae

    # SAFETY CHECKER
    if pipeline.safety_checker is not None:
        safety_checker = pipeline.safety_checker
        clip_num_channels = safety_checker.config.vision_config.num_channels
        clip_image_size = safety_checker.config.vision_config.image_size
        safety_checker.forward = safety_checker.forward_onnx
        onnx_export(
            pipeline.safety_checker,
            model_args=(
                torch.randn(
                    1,
                    clip_num_channels,
                    clip_image_size,
                    clip_image_size,
                ).to(device=device, dtype=dtype),
                torch.randn(1, vae_sample_size, vae_sample_size, vae_out_channels).to(device=device, dtype=dtype),
            ),
            output_path=output_path / "safety_checker" / "model.onnx",
            ordered_input_names=["clip_input", "images"],
            output_names=["out_images", "has_nsfw_concepts"],
            dynamic_axes={
                "clip_input": {0: "batch", 1: "channels", 2: "height", 3: "width"},
                "images": {0: "batch", 1: "height", 2: "width", 3: "channels"},
            },
            opset=opset,
        )
        del pipeline.safety_checker
        safety_checker = OnnxRuntimeModel.from_pretrained(output_path / "safety_checker")
        feature_extractor = pipeline.feature_extractor
    else:
        safety_checker = None
        feature_extractor = None

    onnx_pipeline = OnnxStableDiffusionPipeline(
        vae_encoder=OnnxRuntimeModel.from_pretrained(output_path / "vae_encoder"),
        vae_decoder=OnnxRuntimeModel.from_pretrained(output_path / "vae_decoder"),
        text_encoder=OnnxRuntimeModel.from_pretrained(output_path / "text_encoder"),
        tokenizer=pipeline.tokenizer,
        unet=OnnxRuntimeModel.from_pretrained(output_path / "unet"),
        scheduler=pipeline.scheduler,
        safety_checker=safety_checker,
        feature_extractor=feature_extractor,
        requires_safety_checker=safety_checker is not None,
    )

    onnx_pipeline.save_pretrained(output_path)
    print("ONNX pipeline saved to", output_path)

    del pipeline
    del onnx_pipeline
    _ = OnnxStableDiffusionPipeline.from_pretrained(output_path, provider="CPUExecutionProvider")
    print("ONNX pipeline is loadable")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()

    parser.add_argument(
        "--model_path",
        type=str,
        required=True,
        help="Path to the `diffusers` checkpoint to convert (either a local directory or on the Hub).",
    )

    parser.add_argument("--output_path", type=str, required=True, help="Path to the output model.")

    parser.add_argument(
        "--opset",
        default=14,
        type=int,
        help="The version of the ONNX operator set to use.",
    )
    parser.add_argument("--fp16", action="store_true", default=False, help="Export the models in `float16` mode")

    args = parser.parse_args()

    convert_models(args.model_path, args.output_path, args.opset, args.fp16)
(stable_diffusion_env) boris@sensei:/data/models$ 


ChatGPT
It looks like you're trying to run a script that converts a model to ONNX format, but you're encountering an error because the onnx package is not installed in your virtual environment.

You can install the ONNX package by running the following command in your virtual environment:

bash
Copy code
pip install onnx
Once you've installed the ONNX package, you should be able to run your script without encountering the ModuleNotFoundError.

If you encounter any further issues or have any questions, please let me know!




User
Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease
Hit:2 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease
Hit:3 http://be.archive.ubuntu.com/ubuntu jammy InRelease                
Hit:4 http://be.archive.ubuntu.com/ubuntu jammy-updates InRelease     
Hit:5 https://repo.radeon.com/amdgpu/5.6/ubuntu jammy InRelease
Hit:6 http://be.archive.ubuntu.com/ubuntu jammy-backports InRelease
Hit:7 https://repo.radeon.com/rocm/apt/5.6 jammy InRelease
Reading package lists... Done
Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease
Hit:2 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease
Hit:3 http://be.archive.ubuntu.com/ubuntu jammy InRelease                
Hit:4 http://be.archive.ubuntu.com/ubuntu jammy-updates InRelease     
Hit:5 http://be.archive.ubuntu.com/ubuntu jammy-backports InRelease
Hit:6 https://repo.radeon.com/amdgpu/5.6/ubuntu jammy InRelease
Hit:7 https://repo.radeon.com/rocm/apt/5.6 jammy InRelease
Reading package lists... Done
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
E: Unable to locate package amf-amdgpu-pro
E: Unable to locate package amdgpu-pro
E: Unable to locate package amdgpu-pro-lib32
(stable_diffusion_env) boris@sensei:/data/models$ sudo amdgpu-install --rocmrelease=5.6.0 --usecase=graphics,multimedia,rocm,amf,lrt,opencl,hip,mllib,workstation




ChatGPT
It seems like you're trying to install AMD GPU drivers and ROCm (Radeon Open Compute) but encountering an error with the package names.

Here's a general guide to install ROCm on Ubuntu:

Add the ROCm APT repository:

bash
Copy code
sudo apt install libnuma-dev
sudo wget -qO - http://repo.radeon.com/rocm/rocm.gpg.key | sudo apt-key add -
echo 'deb [arch=amd64] http://repo.radeon.com/rocm/apt/debian/ xenial main' | sudo tee /etc/apt/sources.list.d/rocm.list
Update the APT repository:

bash
Copy code
sudo apt update
Install ROCm:

bash
Copy code
sudo apt install rocm-dkms
Add the user to the video group:

bash
Copy code
sudo usermod -a -G video $LOGNAME
Reboot the system:

bash
Copy code
sudo reboot
Verify the installation:

bash
Copy code
/opt/rocm/bin/rocminfo
/opt/rocm/opencl/bin/x86_64/clinfo
Please note that the above instructions are for a general installation of ROCm. If you have specific requirements or if the above steps do not work, please refer to the official ROCm installation guide for detailed instructions tailored to your system and use case.

If you continue to encounter issues, please provide more details about the specific problem, and I'll do my best to assist you!




User
(stable_diffusion_env) boris@sensei:~$ /opt/rocm/bin/rocminfo
ROCk module is loaded
Unable to open /dev/kfd read-write: Permission denied
boris is not member of "render" group, the default DRM access group. Users must be a member of the "render" group or another DRM access group in order for ROCm applications to run successfully.
(stable_diffusion_env) boris@sensei:~$ sudo usermod -a -G video boris
(stable_diffusion_env) boris@sensei:~$ sudo usermod -a -G render boris
(stable_diffusion_env) boris@sensei:~$ newgrp render
boris@sensei:~$ source stable_diffusion_env/bin/activate
(stable_diffusion_env) boris@sensei:~$ /opt/rocm/bin/rocminfo
ROCk module is loaded
Failed to parse CPUID
=====================    
HSA System Attributes    
=====================    
Runtime Version:         1.1
System Timestamp Freq.:  1000.000000MHz
Sig. Max Wait Duration:  18446744073709551615 (0xFFFFFFFFFFFFFFFF) (timestamp count)
Machine Model:           LARGE                              
System Endianness:       LITTLE                             

==========               
HSA Agents               
==========               
*******                  
Agent 1                  
*******                  
  Name:                    QEMU Virtual CPU version 2.5+      
  Uuid:                    CPU-XX                             
  Marketing Name:          QEMU Virtual CPU version 2.5+      
  Vendor Name:             CPU                                
  Feature:                 None specified                     
  Profile:                 FULL_PROFILE                       
  Float Round Mode:        NEAR                               
  Max Queue Number:        0(0x0)                             
  Queue Min Size:          0(0x0)                             
  Queue Max Size:          0(0x0)                             
  Queue Type:              MULTI                              
  Node:                    0                                  
  Device Type:             CPU                                
  Cache Info:              
    L1:                      65536(0x10000) KB                  
  Chip ID:                 0(0x0)                             
  ASIC Revision:           0(0x0)                             
  Cacheline Size:          64(0x40)                           
  Max Clock Freq. (MHz):   0                                  
  BDFID:                   0                                  
  Internal Node ID:        0                                  
  Compute Unit:            2                                  
  SIMDs per CU:            0                                  
  Shader Engines:          0                                  
  Shader Arrs. per Eng.:   0                                  
  WatchPts on Addr. Ranges:1                                  
  Features:                None
  Pool Info:               
    Pool 1                   
      Segment:                 GLOBAL; FLAGS: FINE GRAINED        
      Size:                    8025592(0x7a75f8) KB               
      Allocatable:             TRUE                               
      Alloc Granule:           4KB                                
      Alloc Alignment:         4KB                                
      Accessible by all:       TRUE                               
    Pool 2                   
      Segment:                 GLOBAL; FLAGS: KERNARG, FINE GRAINED
      Size:                    8025592(0x7a75f8) KB               
      Allocatable:             TRUE                               
      Alloc Granule:           4KB                                
      Alloc Alignment:         4KB                                
      Accessible by all:       TRUE                               
    Pool 3                   
      Segment:                 GLOBAL; FLAGS: COARSE GRAINED      
      Size:                    8025592(0x7a75f8) KB               
      Allocatable:             TRUE                               
      Alloc Granule:           4KB                                
      Alloc Alignment:         4KB                                
      Accessible by all:       TRUE                               
  ISA Info:                
*** Done ***             
(stable_diffusion_env) boris@sensei:~$ /opt/rocm/opencl/bin/x86_64/clinfo
bash: /opt/rocm/opencl/bin/x86_64/clinfo: No such file or directory
(stable_diffusion_env) boris@sensei:~$ 

ChatGPT
It looks like you've successfully added your user to the "render" group, and the rocminfo command is now working. However, it seems that the OpenCL information tool clinfo is not found in the expected location.

Here's what you can do to troubleshoot:

Check if clinfo is installed:

bash
Copy code
clinfo
If it's not installed, you can install it using:

bash
Copy code
sudo apt install clinfo
Check the ROCm OpenCL installation:

It seems that the OpenCL binaries are not found in the expected location. You may want to reinstall the ROCm OpenCL runtime:

bash
Copy code
sudo apt install rocm-opencl
Reboot your system:

If you haven't already, rebooting your system can help apply all the changes and ensure that everything is loaded correctly.

Verify the GPU support:

The rocminfo output shows a QEMU Virtual CPU, which indicates that you might be running this inside a virtual machine. Ensure that your GPU is properly passed through to the virtual machine and that it is supported by ROCm.

Refer to the official documentation:

If you continue to face issues, consult the official ROCm installation guide for detailed instructions and troubleshooting steps tailored to your specific system and GPU.

Feel free to provide more details or ask further questions if you need additional assistance!




User
(stable_diffusion_env) boris@sensei:~$ /opt/rocm/opencl/bin/x86_64/clinfo
bash: /opt/rocm/opencl/bin/x86_64/clinfo: No such file or directory
(stable_diffusion_env) boris@sensei:~$ clinfo
Failed to parse CPUID
Number of platforms                               1
  Platform Name                                   AMD Accelerated Parallel Processing
  Platform Vendor                                 Advanced Micro Devices, Inc.
  Platform Version                                OpenCL 2.1 AMD-APP (3581.0)
  Platform Profile                                FULL_PROFILE
  Platform Extensions                             cl_khr_icd cl_amd_event_callback 
  Platform Extensions function suffix             AMD
  Platform Host timer resolution                  1ns

  Platform Name                                   AMD Accelerated Parallel Processing
Number of devices                                 0

NULL platform behavior
  clGetPlatformInfo(NULL, CL_PLATFORM_NAME, ...)  No platform
  clGetDeviceIDs(NULL, CL_DEVICE_TYPE_ALL, ...)   No platform
  clCreateContext(NULL, ...) [default]            No platform
  clCreateContext(NULL, ...) [other]              
  clCreateContextFromType(NULL, CL_DEVICE_TYPE_DEFAULT)  No devices found in platform
  clCreateContextFromType(NULL, CL_DEVICE_TYPE_CPU)  No devices found in platform
  clCreateContextFromType(NULL, CL_DEVICE_TYPE_GPU)  No devices found in platform
  clCreateContextFromType(NULL, CL_DEVICE_TYPE_ACCELERATOR)  No devices found in platform
  clCreateContextFromType(NULL, CL_DEVICE_TYPE_CUSTOM)  No devices found in platform
  clCreateContextFromType(NULL, CL_DEVICE_TYPE_ALL)  No devices found in platform
(stable_diffusion_env) boris@sensei:~$ 

ChatGPT
The output of clinfo indicates that the AMD OpenCL platform is detected, but no devices are found. This could be due to several reasons:

GPU Compatibility: Ensure that the GPU in your system is compatible with the ROCm version you are using. Not all AMD GPUs are supported, and compatibility may vary between ROCm versions.

Virtual Machine Environment: If you are running this inside a virtual machine, make sure that the GPU is properly passed through to the VM. Virtualized environments can have additional complexities when it comes to GPU access.

Driver Configuration: It's possible that the ROCm driver is not properly configured or that there is a conflict with another driver. You may want to try reinstalling ROCm or checking the system logs for any related errors.

Kernel Version: ROCm has specific requirements for the Linux kernel version. Ensure that your kernel version is compatible with the ROCm version you are using.

Reboot the System: If you haven't already, rebooting the system can help apply all the changes and ensure that everything is loaded correctly.

Consult ROCm Documentation: The official ROCm installation guide may have specific instructions or troubleshooting steps for your particular system and GPU.

Check System Logs: You may find additional information in the system logs. Try running:

bash
Copy code
dmesg | grep kfd
This command will filter the kernel log for messages related to the ROCm Kernel Fusion Driver (KFD), which may provide insights into what's going wrong.

Verify BIOS Settings: Sometimes, certain BIOS settings related to virtualization or GPU configuration may affect ROCm's ability to detect the GPU. Check your BIOS settings to ensure they are configured correctly.

Without more specific information about your system and GPU, these are general troubleshooting steps you can take. If you continue to face issues, please provide more details about your system, such as the GPU model, ROCm version, and whether you are running in a virtualized environment, so I can assist you further.




User
I am buying a new GPU, which one would you advice to be able to run StabilityAI's Stable Diffusion XL v1 on linux?

2 / 2


ChatGPT
StabilityAI's Stable Diffusion XL v1 likely has specific hardware requirements, especially if it's designed to perform complex computations like deep learning or scientific simulations. While I don't have specific details about StabilityAI's requirements, I can provide general guidance on selecting a GPU for Linux that should be suitable for most high-performance computing tasks.

Check StabilityAI's Documentation: If available, consult the official documentation or support channels for StabilityAI's Stable Diffusion XL v1. They may have specific recommendations or requirements for GPUs.

Consider AMD GPUs with ROCm Support: Since you were working with ROCm, you might want to consider AMD GPUs that are officially supported by ROCm. AMD's Radeon RX series and Radeon Instinct series are popular choices for scientific and deep learning tasks. Check the ROCm compatibility list to find a GPU that matches your ROCm version.

Consider NVIDIA GPUs with CUDA Support: NVIDIA's GPUs are widely used in the deep learning and scientific computing communities. The NVIDIA Titan, Tesla, and GeForce RTX series are all strong choices. NVIDIA provides excellent Linux support through their CUDA platform.

Evaluate Your Specific Needs: Consider the specific computations and workloads you'll be running. If you need high double-precision performance, look for GPUs that excel in that area. If deep learning is your focus, single-precision and tensor core performance might be more important.

Consider Memory Requirements: For large models or datasets, GPU memory can be a limiting factor. Consider GPUs with a large amount of VRAM (e.g., 16GB or more) if you anticipate working with large data.

Budget Considerations: High-performance GPUs can be expensive. Balance your performance needs with your budget. Sometimes, a slightly older model can provide excellent performance at a reduced cost.

Linux Compatibility: Ensure that the GPU you choose is well-supported on your specific Linux distribution. Check forums, community feedback, and official documentation.

Power and Cooling: High-performance GPUs can consume significant power and generate substantial heat. Ensure that your system has a sufficient power supply and cooling solution.

Warranty and Support: Consider purchasing from a vendor that offers good warranty and support, especially if you're investing in a high-end GPU.

Scalability: If you plan to scale your system in the future, consider GPUs that can be used in multi-GPU configurations.

Without specific details about StabilityAI's Stable Diffusion XL v1, these are general guidelines. If you can provide more information about the software's requirements or your specific use case, I can offer more tailored advice.




User
INNO3D GeForce RTX 3060 Twin X2 LHR Videokaart
Uit eigen voorraad leverbaar. Levertijd: 1 dag (zaterdag)
MerkINNO3D
Graphics EngineGeForce RTX 3060
Videogeheugen12 GB
GPU snelheid (max)1777 MHz
CUDA cores3584
VGA Geheugen typeGDDR6
LHRcheck icon
EAN4895223102091
VendorcodeN30602-12D6-119032AH
Garantie36 Maanden
Meer productinformatie
287,90

ChatGPT
The INNO3D GeForce RTX 3060 Twin X2 LHR is a mid-range GPU that's part of NVIDIA's RTX 30 series. It comes with 12GB of GDDR6 memory and 3584 CUDA cores, and it supports NVIDIA's latest graphics technologies, including ray tracing.

Here's how it stacks up against the considerations I mentioned earlier:

Performance: With 3584 CUDA cores and a maximum GPU speed of 1777 MHz, this card should offer good performance for many deep learning and scientific computing tasks. However, it may not be suitable for the most demanding workloads.

Memory: 12GB of GDDR6 memory is a decent amount for many applications, but very large models or datasets might require more.

LHR (Lite Hash Rate): Note that this is an LHR model, which means it has a reduced hash rate for cryptocurrency mining. This won't affect its performance in other tasks but may affect resale value if you ever decide to sell the card.

Linux Compatibility: NVIDIA generally provides good Linux support, and the RTX 3060 should be well-supported. However, it's always a good idea to check compatibility with your specific Linux distribution.

Power and Cooling: Make sure your system has a sufficient power supply and cooling solution to handle this card.

Price: At €287.90, this seems like a reasonable price for an RTX 3060, especially given the current GPU market.

Warranty: A 36-month warranty is a solid offering and should provide some peace of mind.

Without knowing the specific requirements of StabilityAI's Stable Diffusion XL v1, it's hard to say definitively whether this card will meet your needs. If the software's documentation or support channels provide any specific GPU recommendations or requirements, be sure to check those against the specs of this card.

If your workloads are not extremely demanding, and you're looking for a good value GPU that can handle a variety of tasks, the INNO3D GeForce RTX 3060 Twin X2 LHR could be a good choice. If you anticipate running very large models or need the highest possible performance, you might want to consider a higher-end option.




User
how do I change git user?

ChatGPT
Changing the Git user involves configuring the username and email address that will be associated with your commits. Here's how you can do it:

To Change Git User Globally
If you want to change the Git user for all repositories on your system, you can use the following commands:

Open a terminal.
Run the following command to set your username:
bash
Copy code
git config --global user.name "Your Name"
Run the following command to set your email address:
bash
Copy code
git config --global user.email "youremail@example.com"
To Change Git User for a Specific Repository
If you want to change the Git user only for a specific repository, navigate to that repository's directory in the terminal and run the following commands:

Run the following command to set your username:
bash
Copy code
git config user.name "Your Name"
Run the following command to set your email address:
bash
Copy code
git config user.email "youremail@example.com"
Verifying the Changes
You can verify that the changes were made successfully by running the following commands:

To check the global username and email:

bash
Copy code
git config --global user.name
git config --global user.email
To check the repository-specific username and email:

bash
Copy code
git config user.name
git config user.email
These commands will print the configured name and email address to the terminal.

Keep in mind that these changes will only affect future commits. Past commits will retain the previous username and email configuration. If you need to change the user information for previous commits, that's a more complex operation and should be done with caution, especially if the commits have already been pushed to a remote repository.




User
boris@boris-ThinkPad-T460:~/pirate-party$ git status
On branch Vlaams
Your branch is ahead of 'origin/Vlaams' by 1 commit.
  (use "git push" to publish your local commits)

nothing to commit, working tree clean
boris@boris-ThinkPad-T460:~/pirate-party$ git push
remote: Permission to b-truyens/pirate-party.git denied to knopwobble.
fatal: unable to access 'https://github.com/b-truyens/pirate-party/': The requested URL returned error: 403
boris@boris-ThinkPad-T460:~/pirate-party$ git user
git: 'user' is not a git command. See 'git --help'.

The most similar command is
        subtree
boris@boris-ThinkPad-T460:~/pirate-party$ git --help
usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]
           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]
           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]
           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]
           [--super-prefix=<path>] [--config-env=<name>=<envvar>]
           <command> [<args>]

These are common Git commands used in various situations:

start a working area (see also: git help tutorial)
   clone     Clone a repository into a new directory
   init      Create an empty Git repository or reinitialize an existing one

work on the current change (see also: git help everyday)
   add       Add file contents to the index
   mv        Move or rename a file, a directory, or a symlink
   restore   Restore working tree files
   rm        Remove files from the working tree and from the index

examine the history and state (see also: git help revisions)
   bisect    Use binary search to find the commit that introduced a bug
   diff      Show changes between commits, commit and working tree, etc
   grep      Print lines matching a pattern
   log       Show commit logs
   show      Show various types of objects
   status    Show the working tree status

grow, mark and tweak your common history
   branch    List, create, or delete branches
   commit    Record changes to the repository
   merge     Join two or more development histories together
   rebase    Reapply commits on top of another base tip
   reset     Reset current HEAD to the specified state
   switch    Switch branches
   tag       Create, list, delete or verify a tag object signed with GPG

collaborate (see also: git help workflows)
   fetch     Download objects and refs from another repository
   pull      Fetch from and integrate with another repository or a local branch
   push      Update remote refs along with associated objects

'git help -a' and 'git help -g' list available subcommands and some
concept guides. See 'git help <command>' or 'git help <concept>'
to read about a specific subcommand or concept.
See 'git help git' for an overview of the system.
boris@boris-ThinkPad-T460:~/pirate-party$ git config --global user.name
git config --global user.email
b-truyens
boristruyens@protonmail.com
boris@boris-ThinkPad-T460:~/pirate-party$ git config user.name
git config user.email
b-truyens
boristruyens@protonmail.com
boris@boris-ThinkPad-T460:~/pirate-party$ git add .
boris@boris-ThinkPad-T460:~/pirate-party$ git commit -m "changes"
On branch Vlaams
Your branch is ahead of 'origin/Vlaams' by 1 commit.
  (use "git push" to publish your local commits)

nothing to commit, working tree clean
boris@boris-ThinkPad-T460:~/pirate-party$ git push
remote: Permission to b-truyens/pirate-party.git denied to knopwobble.
fatal: unable to access 'https://github.com/b-truyens/pirate-party/': The requested URL returned error: 403
boris@boris-ThinkPad-T460:~/pirate-party$ 

ChatGPT
It looks like you are trying to push changes to a repository where you don't have write access. The error message Permission to b-truyens/pirate-party.git denied to knopwobble indicates that the GitHub user knopwobble does not have permission to push to the b-truyens/pirate-party repository.

Here's what you can do to resolve the issue:

Check the Remote URL: Make sure you are pushing to the correct remote repository. You can check the remote URL with the following command:

bash
Copy code
git remote -v
Change the Remote URL: If the remote URL is incorrect, you can change it to the correct one using the following command (replace new-url with the correct URL):

bash
Copy code
git remote set-url origin new-url
Use the Correct Account: If you are trying to push to a repository that you own or have write access to, make sure you are authenticated with the correct GitHub account. You may need to re-authenticate with the correct credentials.

Fork the Repository: If you are trying to contribute to someone else's repository, you may need to fork the repository to your own GitHub account, clone the fork, make your changes, and then create a pull request.

Contact the Repository Owner: If you believe you should have write access to the repository, you may need to contact the repository owner to request the necessary permissions.

Re-Clone with SSH: If you are still facing issues, you might want to clone the repository using SSH instead of HTTPS. This often resolves authentication issues. Here's how you can do it:

On GitHub, navigate to the repository and click the "Code" button.
Select "SSH" and copy the SSH URL.
Clone the repository using the SSH URL:
bash
Copy code
git clone ssh-url
Please note that changing the Git user (as you did with git config user.name and git config user.email) only changes the author information for commits. It does not affect authentication or permissions for pushing to remote repositories. Authentication is typically handled through SSH keys or HTTPS credentials, depending on how you have Git configured.




Send a message


ChatGPT may produce inaccurate information about people, places, or facts. ChatGPT August 3 Version
ChatGPT
